{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca27722-8ce1-415f-8a6f-64b1991196f7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdf75da-5b76-4466-9309-aa11649bf66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import whisper   \n",
    "from gtts import gTTS\n",
    "import os\n",
    "from tempfile import NamedTemporaryFile\n",
    "from transformers import pipeline\n",
    "from langchain.llms import Ollama\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e28c7d-e4b1-4611-a7d2-d649c43a7cc7",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f49f74e-ba74-455d-ba88-90c520de64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d268c41-8ab8-4a6e-8502-9d08a12d9265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "intent_pipeline = pipeline(\"text-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3745290-bc74-4559-acb4-59b2cbc2f9e0",
   "metadata": {},
   "source": [
    "## Create Vector-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e8c4754-d007-4002-8ef4-d2915d551763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1088, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"llama3.2\"\n",
    "db_name = \"vector_db\"\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "embeddings = GPT4AllEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072bca93-e7e7-4a6a-8b2d-1647ef355374",
   "metadata": {},
   "source": [
    "## Set LLM for Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fdbdb52-434d-4443-8982-88ab6228e536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhudi\\AppData\\Local\\Temp\\ipykernel_13516\\1068236361.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(base_url=\"http://localhost:11434\", model=MODEL)\n",
      "C:\\Users\\bhudi\\AppData\\Local\\Temp\\ipykernel_13516\\1068236361.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(base_url=\"http://localhost:11434\", model=MODEL)\n",
    "retriever = vectorstore.as_retriever()\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe220a7-1105-4e74-83f8-b7f282c1d11f",
   "metadata": {},
   "source": [
    "## Manual Setup for ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f968f7e-dc09-4624-b8e7-603c10336bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:/Users/bhudi/Downloads/ffm/ffmpeg-2025-02-20-git-bc1a3bfd2c-essentials_build/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ace0e2-2867-4c99-a890-44840427d1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg is installed and accessible\n"
     ]
    }
   ],
   "source": [
    "def speech_to_text(audio_path):\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(audio_path):\n",
    "            raise FileNotFoundError(f\"Audio file not found at: {audio_path}\")\n",
    "            \n",
    "        # Check if ffmpeg is available\n",
    "        import subprocess\n",
    "        try:\n",
    "            subprocess.run(['ffmpeg', '-version'], capture_output=True)\n",
    "        except FileNotFoundError:\n",
    "            raise RuntimeError(\"ffmpeg not found. Please install ffmpeg and add it to PATH\")\n",
    "            \n",
    "        # Now try transcription\n",
    "        transcription = model.transcribe(audio_path)[\"text\"]\n",
    "        return transcription\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Detailed error in speech_to_text: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def verify_ffmpeg():\n",
    "    \"\"\"Verify ffmpeg installation\"\"\"\n",
    "    import subprocess\n",
    "    try:\n",
    "        result = subprocess.run(['ffmpeg', '-version'], \n",
    "                              capture_output=True, \n",
    "                              text=True)\n",
    "        print(\"ffmpeg is installed and accessible\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(\"ffmpeg is NOT installed or not in PATH\")\n",
    "        return False\n",
    "\n",
    "# Add this check when starting your app\n",
    "if not verify_ffmpeg():\n",
    "    print(\"Please install ffmpeg before running this application\")\n",
    "    print(\"You can install it using: choco install ffmpeg\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45b703-9078-4010-8023-f08f63ef92b6",
   "metadata": {},
   "source": [
    "## Function Definations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d8af59b-30a5-4c5a-926b-4de9a0a23695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_to_text(audio_path):\n",
    "    transcription = model.transcribe(audio_path)[\"text\"]\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a91c8a2-b309-4015-950c-c6379e8ba5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_intent_and_entities(text):\n",
    "    entities = ner_pipeline(text)\n",
    "    intent = intent_pipeline(text)[0]['label']\n",
    "    return intent, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd8bff9a-9283-4b2a-9f3a-3a093dd5973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(text):\n",
    "    result = conversation_chain.invoke({\"question\": text})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1c56d72-2c6e-44ec-b6aa-12a0bf6d848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, lang=\"en\", slow=False):\n",
    "    tts = gTTS(text, lang=lang, slow=slow)\n",
    "    output_audio = NamedTemporaryFile(suffix=\".mp3\", delete=False)\n",
    "    tts.save(output_audio.name)\n",
    "    return output_audio.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff1177b1-06aa-4423-9100-5e82731bbb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_pipeline(audio_path=None, text_input=None):\n",
    "    try:\n",
    "        if audio_path:\n",
    "            print(f\"Received audio path: {audio_path}\")\n",
    "            if not os.path.exists(audio_path):\n",
    "                return \"Error: Audio file not found\", None, None, None\n",
    "            text_input = speech_to_text(audio_path)\n",
    "            print(f\"Transcription: {text_input}\")\n",
    "        \n",
    "        if not text_input:\n",
    "            return \"Error: No input provided\", None, None, None\n",
    "        \n",
    "        intent, entities = analyze_intent_and_entities(text_input)\n",
    "        print(f\"Detected Intent: {intent}\")\n",
    "        print(f\"Extracted Entities: {entities}\")\n",
    "        \n",
    "        response_text = generate_response(text_input)\n",
    "        print(f\"Generated Response: {response_text}\")\n",
    "        \n",
    "        response_audio_path = text_to_speech(response_text)\n",
    "        print(f\"Response Audio Path: {response_audio_path}\")\n",
    "        \n",
    "        return response_text, response_audio_path, intent, entities\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90899982-eed3-462b-9670-f5151fbfe89e",
   "metadata": {},
   "source": [
    "## Gradio User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df64cb8e-d73a-4354-a6b8-8604bd011511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhudi\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Intent: neutral\n",
      "Extracted Entities: []\n",
      "Generated Response: Based on the provided context, here is an overview of Avery Lancaster:\n",
      "\n",
      "Avery Lancaster has been a key figure in the growth and success of Insurellm, an insurance technology company. She has demonstrated resilience, adaptability, and leadership skills throughout her career, navigating challenges such as market competition, pandemic-related operational difficulties, and employee concerns.\n",
      "\n",
      "Avery's tenure at Insurellm spans from 2015 to present, with notable achievements including:\n",
      "\n",
      "* Launching two successful products that significantly increased market share in 2018\n",
      "* Regaining market leadership through innovative approaches to personalized insurance solutions in 2023\n",
      "* Fostering a commitment to diversity and inclusion, improving team representation since 2021\n",
      "* Implementing flexible working conditions and regular check-ins to address work-life balance concerns\n",
      "\n",
      "Avery's professional development has been significant, with participation in leadership training programs, industry conferences, and community outreach efforts. She has also received recognition as a leading voice in Insurance Tech innovation.\n",
      "\n",
      "Prior to joining Insurellm, Avery worked as a Business Analyst at Edge Analytics from 2010-2013, which laid the groundwork for her future entrepreneurial endeavors.\n",
      "\n",
      "Throughout her career, Avery has shown ability to pivot and adapt to changing circumstances, leading to improved performance and recognition.\n",
      "Response Audio Path: C:\\Users\\bhudi\\AppData\\Local\\Temp\\tmpmrctkpzl.mp3\n",
      "Detected Intent: neutral\n",
      "Extracted Entities: []\n",
      "Generated Response: I don't know. The context only mentions \"Carllm\" as the name of an auto insurance product and platform, but it doesn't provide any information about a separate entity or person named \"Carl\".\n",
      "Response Audio Path: C:\\Users\\bhudi\\AppData\\Local\\Temp\\tmpsc07tkj5.mp3\n",
      "Detected Intent: neutral\n",
      "Extracted Entities: []\n",
      "Generated Response: Insurellm is an insurance tech startup that was founded by Avery Lancaster in 2015. Its first product, Markellm, is a marketplace connecting consumers with insurance providers. The company has expanded rapidly, adding new products and clients, and reaching 200 employees by 2024 with 12 offices across the US.\n"
     ]
    }
   ],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=chatbot_pipeline,\n",
    "    inputs=[\n",
    "        gr.Audio(type=\"filepath\", label=\"Speak\"),\n",
    "        gr.Textbox(label=\"Type your query\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Response Text\"),\n",
    "        gr.Audio(label=\"Response Audio\"),\n",
    "        gr.Textbox(label=\"Detected Intent\"),\n",
    "        gr.Textbox(label=\"Extracted Entities\")\n",
    "    ],\n",
    "    title=\"NLU-Enhanced RAG Chatbot with Voice & Text\",\n",
    "    allow_flagging='never'\n",
    ")\n",
    "\n",
    "iface.launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bdb897-a458-469e-a5fa-81bb23342cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
